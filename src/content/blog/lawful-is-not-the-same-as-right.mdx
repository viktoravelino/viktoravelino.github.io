---
title: "Lawful Is Not the Same as Right"
description: "Anthropic just told the Department of War no — twice. What happens when the company behind your daily tools has to draw lines with the most powerful customer imaginable."
date: 2026-02-27
tags: ["ai", "ethics", "software-engineering"]
coverImage: "/images/blog/lawful-is-not-the-same-as-right.png"
draft: true
---

import ArticleCard from "../../components/ArticleCard.astro";

Yesterday, Dario Amodei — CEO of Anthropic, the company that makes Claude, the AI I literally use to write code every day — published a statement about their relationship with the Department of War.

The short version: Anthropic has been doing extensive national security work. Claude is deployed on classified government networks, used for intelligence analysis, operational planning, cyber operations. They were the first frontier AI company to do this. They also walked away from hundreds of millions in revenue by cutting off access to firms linked to the Chinese Communist Party.

And then the Department of War asked for two more things, and Anthropic said no.

---

## The Two Lines

The first is mass domestic surveillance. Current U.S. law allows the government to buy citizens' movement data, browsing history, and association records without a warrant. That's already unsettling. But Amodei's argument is that AI changes the equation — it can take scattered, individually mundane data points and assemble them into something that feels a lot like a comprehensive profile of your life. Legal doesn't mean harmless.

The second is fully autonomous weapons. Not the partially autonomous systems already in use — Anthropic's fine with those. The line is at weapons that remove humans from targeting decisions entirely. Their position is straightforward: frontier AI isn't reliable enough for that. They offered to collaborate on R&D to get there safely. The Department said no, we want it now, no safeguards.

---

## "Any Lawful Use"

The Department's response was to demand "any lawful use" without restrictions. When Anthropic held firm, the threats came: removal from government contracts, "supply chain risk" designations, invocation of the Defense Production Act.

There's something almost darkly funny about simultaneously threatening to label a company a supply chain risk *and* demanding they provide unrestricted access to their technology. As Amodei put it, the positions are "inherently contradictory."

---

## Why This Matters to Developers

Here's why I think this is worth reading even if you don't follow defense policy.

Anthropic makes the tools a lot of us use daily. Claude Code is in my workflow every single day. When I read about what the company behind that tool decides to do — and not do — with their most powerful and most dangerous customer, it lands differently than reading about some abstract policy debate.

This is the tension at the center of building anything useful. The thing you build will be used in ways you didn't intend. And at some point you have to decide where your line is — not where the law says it is, but where *you* say it is.

"Any lawful use" is a framework that sounds reasonable until you think about it for thirty seconds. Plenty of terrible things are lawful. The entire history of tech ethics is basically the story of companies learning that "we didn't break any laws" is not the defense they thought it was.

---

## The Uncomfortable Part

I'll be honest — there's something uncomfortable about this too. Anthropic isn't saying no to defense work. They're deeply embedded in it. They're saying no to *these two specific things* while actively supporting intelligence analysis and cyber operations. You can read that as principled restraint or as selective ethics depending on your priors.

But I'd rather have a company drawing lines — even imperfect, debatable ones — than one that just hands over the keys and says "anything legal goes." The alternative to imperfect boundaries isn't perfect boundaries. It's no boundaries at all.

And in a world where AI companies are about to become some of the most consequential infrastructure providers on the planet, the question of who gets to decide what these tools do — and who gets to say no — matters a lot more than it did when the biggest debate was whether your chatbot should write your homework.

Lawful is not the same as right. It never has been. It's good that at least someone's saying it out loud.

---

## Further Reading

<ArticleCard
  url="https://www.anthropic.com/news/statement-department-of-war"
  title="Statement from Dario Amodei on Department of War Discussions"
  description="Anthropic's CEO outlines the company's position on AI deployment for national security, including two non-negotiable safeguards on domestic surveillance and autonomous weapons."
  source="anthropic.com"
/>
